{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "import tensorflow as tf\n",
    "import altair as alt\n",
    "import collections\n",
    "\n",
    "from src.load_data import read_tables\n",
    "from src.item_recommender import ItemRecommender\n",
    "from src.CF_Softmax_Model import CFModel, build_CF_model, compute_scores, book_neighbors, user_recommendations, split_dataframe\n",
    "from src.Baseline_Model import build_baseline_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reviews, books = read_tables('data', 'goodreads_reviews_mystery_thriller_crime.json.gz', 'goodreads_books_mystery_thriller_crime.json.gz')\n",
    "# reviews.to_csv('data/cleaned_reviews_mystery_1.csv', header=reviews.columns, index=False)\n",
    "# books.to_csv('data/cleaned_books_mystery_1.csv', header=books.columns, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('data/cleaned_reviews_mystery_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.read_csv('data/cleaned_books_mystery_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "books.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reviews.user_id.unique()), len(reviews.book_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(books.author_id.unique()), len(books.publisher.unique()), len(books.book_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE = False\n",
    "if SAMPLE:\n",
    "    N = 10000\n",
    "    reviews = reviews[(reviews['user_id']<N) & (reviews['book_id']<N)]\n",
    "    books = books[books['book_id'] < N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some of the tensorflow code assumes the ids are string\n",
    "reviews['book_id'] = reviews['book_id'].astype(str)\n",
    "reviews['user_id'] = reviews['user_id'].astype(str)\n",
    "books['book_id'] = books['book_id'].astype(str)\n",
    "books[\"author_id\"] = books[\"author_id\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>[10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>[287]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>[404]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49</td>\n",
       "      <td>[544]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>[354]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id book_id\n",
       "0        7    [10]\n",
       "1       11   [287]\n",
       "2       19   [404]\n",
       "3       49   [544]\n",
       "4       64   [354]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create rated book_id list for each user\n",
    "rated_books =(reviews[['user_id', 'book_id']].groupby('user_id', as_index=False).aggregate(lambda x: list(x)))\n",
    "rated_books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_dict = {\n",
    "    book: author for book, author in zip(books[\"book_id\"], books[\"author_id\"])\n",
    "}\n",
    "publisher_dict = {\n",
    "    book: publisher\n",
    "    for book, publisher in zip(books[\"book_id\"], books[\"publisher\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = [['82014', '83650', '20417', '84776', '65947', '3912', '1016', '23365', '2724', '12991', '21534', '44449', '41186', '95668', '93301'], ['17124', '43808', '52912', '55484'], ['97224', '80190', '82014', '73302', '70630', '90870', '10395', '21830', '86178', '98208', '94335', '20679', '11652', '44449'], ['47263', '10395', '55466'], ['58826', '60026', '60347', '60346', '92204', '79328', '101791', '96559', '100295', '1846', '88505', '95519', '36745', '9954', '82901', '86968', '16069', '104449', '75268']]\n",
    "\n",
    "t = pd.DataFrame.from_dict(book)\n",
    "print(t.head())\n",
    "t.fillna('').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(ratings, batch_size):\n",
    "    \"\"\"Creates a batch of examples.\n",
    "    Args:\n",
    "        ratings: A DataFrame of ratings such that examples[\"book_id\"] is a list of\n",
    "        books rated by a user.\n",
    "    batch_size: The batch size.\n",
    "    \"\"\"\n",
    "    def pad(x, fill):\n",
    "        return pd.DataFrame.from_dict(x).fillna(fill).values\n",
    "\n",
    "    book = []\n",
    "    author = []\n",
    "    publisher = []\n",
    "    label = []\n",
    "    print('make_batch#1')\n",
    "    for book_ids in ratings[\"book_id\"].values:\n",
    "        book_ids = book_ids[:10]\n",
    "        book.append(book_ids)\n",
    "        author.append([author_dict[book_id] for book_id in book_ids])\n",
    "        publisher.append([publisher_dict[book_id] for book_id in book_ids])\n",
    "        label.append([int(book_id) for book_id in book_ids])\n",
    "    print('make_batch#2') \n",
    "    print('book', book[:5])\n",
    "    features = {\n",
    "      \"book_id\": pad(book, \"\"),\n",
    "      #\"author_id\": pad(author, \"\"),\n",
    "      #\"publisher\": pad(publisher, \"\"),\n",
    "      \"label\": pad(label, -1)\n",
    "      }\n",
    "    print('make_batch#3')    \n",
    "    batch = (\n",
    "      tf.data.Dataset.from_tensor_slices(features)\n",
    "      .shuffle(1000)\n",
    "      .repeat()\n",
    "      .batch(batch_size)\n",
    "      .make_one_shot_iterator()\n",
    "      .get_next())\n",
    "    print('make_batch#4')    \n",
    "    return batch\n",
    "\n",
    "def select_random(x):\n",
    "    \"\"\"Selectes a random elements from each row of x.\"\"\"\n",
    "    def to_float(x):\n",
    "        return tf.cast(x, tf.float32)\n",
    "    def to_int(x):\n",
    "        return tf.cast(x, tf.int64)\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    rn = tf.range(batch_size)\n",
    "    nnz = to_float(tf.count_nonzero(x >= 0, axis=1))\n",
    "    rnd = tf.random_uniform([batch_size])\n",
    "    ids = tf.stack([to_int(rn), to_int(nnz * rnd)], axis=1)\n",
    "    return to_int(tf.gather_nd(x, ids))\n",
    "\n",
    "t = make_batch(rated_books, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(user_embeddings, book_embeddings, labels):\n",
    "    user_emb_dim = user_embeddings.shape[1].value\n",
    "    book_emb_dim = book_embeddings.shape[1].value\n",
    "    if user_emb_dim != book_emb_dim:\n",
    "        raise ValueError('The user embedding dimension %d should match the book embedding dimension %d' %(\n",
    "                user_emb_dim, book_emb_dim))\n",
    "    logits = tf.matmul(user_embeddings, book_embeddings, transpose_b=True)\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_softmax_model(rated_books, embedding_cols, hidden_dims, learning_rate =1,):\n",
    "    print('build_softmax_model#1')\n",
    "    def create_network(features):\n",
    "        #create a bog-of-words embedding for each sparse feature\n",
    "        inputs = tf.feature_column.input_layer(features, embedding_cols)\n",
    "        #hidden layer\n",
    "        input_dim = inputs.shape[1].value\n",
    "        for i, output_dim in enumerate(hidden_dims):\n",
    "            w = tf.get_variable(\n",
    "                'hidden%d_w_'% i, shape=[input_dim, output_dim],\n",
    "                initializer=tf.truncated_normal_initializer(\n",
    "                stddev=1./np.sqrt(output_dim)))/10\n",
    "            outputs = tf.matmul(inputs, w)\n",
    "            input_dim = output_dim\n",
    "            inputs = outputs\n",
    "        return outputs\n",
    "    \n",
    "    train_rated_books, test_rated_books = split_dataframe(rated_books)\n",
    "    print('build_softmax_model#2')    \n",
    "    train_batch = make_batch(train_rated_books, 200)\n",
    "    test_batch = make_batch(test_rated_books, 100)\n",
    "    print('build_softmax_model#3')    \n",
    "    with tf.variable_scope('model', reuse=False):\n",
    "        #train\n",
    "        train_user_embeddings =create_network(train_batch)\n",
    "        train_labels = select_random(train_batch['label'])\n",
    "        \n",
    "    with tf.variable_scope('model', reuse=True):\n",
    "        #test\n",
    "        test_user_embeddings = create_network(test_batch)\n",
    "        test_labels = select_random(test_batch['label'])\n",
    "        \n",
    "        book_embeddings = tf.get_variable(\"input_layer/book_id_embedding/embedding_weights\")\n",
    "    print('build_softmax_model#4')        \n",
    "    train_loss = softmax_loss(train_user_embeddings, book_embeddings, train_labels)\n",
    "    test_loss = softmax_loss(test_user_embeddings, book_embeddings, test_labels)\n",
    "    \n",
    "    _, test_prediction_at_10 = tf.metrics.precision_at_k(\n",
    "        labels=test_labels, predictions=tf.matmul(test_user_embeddings, book_embeddings, transpose_b=True),\n",
    "        k=10)\n",
    "    \n",
    "    metrics=(\n",
    "        {'train_loss': train_loss, 'test_loss': test_loss},\n",
    "        {'test_precision_at_10': test_prediction_at_10})\n",
    "    embeddings = {'book_id': book_embeddings}\n",
    "    return CFModel(embeddings, train_loss, metrics, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books['book_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding_col(key, embedding_dim):\n",
    "    categorical_col = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        key=key, vocabulary_list=list(set(books[key].values)), num_oov_buckets=0)\n",
    "    return tf.feature_column.embedding_column(categorical_column=categorical_col, dimension=embedding_dim, combiner='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    print('embedding_cols start')\n",
    "    embedding_cols = [\n",
    "            make_embedding_col('book_id', 5),\n",
    "             #make_embedding_col('author_id',10),\n",
    "             #make_embedding_col('publisher',10)\n",
    "        ]    \n",
    "    print('embedding_cols done')\n",
    "    softmax_model = build_softmax_model(\n",
    "        rated_books, \n",
    "        embedding_cols = embedding_cols, hidden_dims=[5],\n",
    "        learning_rate = 0.1\n",
    "    )\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_model.train( num_iterations =1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
